\appendix

\chapter{Relevant code}


\begin{verbatim}
Optimize (x,s,lambda)

	D23SqInv = spdiags(1./ ...
				( ... 
					x(1+dim:2*dim)./s(1+dim:2*dim)+ ...
					x(1+2*dim:3*dim)./s(1+2*dim:3*dim)  ...
				) ...
				, 0, dim, dim);
	D23SqInv(isnan(D23SqInv)) = 0;
	
	D3 = spdiags(x(1:dim)./s(1:dim), 0, dim, dim);
	D2 = spdiags(x(1:dim)./s(1:dim), 0, dim, dim);
	D1Inv  = spdiags(s(1:dim)./x(1:dim), 0, dim, dim);        
	XInv = spdiags(1./x(:),0,3*dim, 3*dim);
	
	D3(isnan(D3)) = 0;
	D2(isnan(D2)) = 0;
	D1Inv(isnan(D1Inv)) = 0;
	XInv(isnan(XInv)) = 0;
	
	miu = mean(x(:).*s(:));
	sigma = std2(x(:).*s(:));
	ra = x(:).* s(:) - miu*sigma*ones(3*dim, 1);
	rc =  G*x+c-s-A'*l ;
	rb = A*x;
	rch =rc + XInv* ra;
	
	rch1 = rch(1:dim);
	rch2 = rch(1+dim:2*dim);
	rch3 = rch(1+2*dim:3*dim);
	
	rbh = rb + D2 * rch2 - D3 * rch3;
	rcTilda =rch1 +bfK'*(D23SqInv)*rbh;
	
	BigA = 2*G(1:dim, 1:dim)+D1Inv+bf2K'*D23SqInv*K;
	
	deltaF = gmres(BigA, -rcTilda);
	
	
	deltaL = D23SqInv*(-rbh-bfK*deltaF);
	deltaVp = D2*(-rch2-deltaL);
	deltaVm = D3*(-rch3-deltaL);
	deltaX = [deltaF; deltaVp; deltaVm];
	deltaS = G * deltaX- A'*deltaL+ rc;
	
	x = x + deltaX;
	s = s + deltaS;
	l = l + deltaL;


\end{verbatim}

\chapter{Other relevant information (demonstrations, etc.)}
\section{Horn Scunk equation to Jacobi Iteration} \label{GSDemo}
First, let us define the staring equation.
\begin{equation}
	E = (I_xu+ I_yv + I_t)^2 + \lambda(\left\Vert(\nabla u) \right\Vert_2^2 +\left\Vert(\nabla v)\right\Vert_2^2)
\end{equation}
by discretizing $\left\Vert(\nabla u) \right\Vert_2^2$ with a Laplace filter, it can be rewritten as $(\bar{u}-u)^2$. The equation then becomes:
\begin{equation}
E = (I_xu+ I_yv + I_t)^2 + \lambda((\bar{u}-u)^2+(\bar{v}-v)^2)
\end{equation}
The we apply the partial derivatives and eqaual then to 0 in order to find the minimum, as E being convex,

\begin{equation} 
\begin{split} 
\frac{\partial E}{\partial u} = I_x(I_xu+I_yv+Iy) + \lambda(\bar{u}-u) \\
\frac{\partial E}{\partial v} = I_y(I_xu+I_yv+Iy) + \lambda(\bar{v}-v)
\end{split}
\end{equation}
Now to rearrange the  terms
\begin{equation} 
\begin{split} 
(I_x^2+\lambda)u + I_y I_x v  =  -I_xI_t + \lambda \bar{u} \\
I_y I_x u+(I_y^2+\lambda)v  = -I_yI_t + \lambda \bar{v} \\
\end{split}
\end{equation}
and in matrix form , $A \boldsymbol{x} = b$:

\begin{equation}
	\begin{bmatrix}
	I_x^2+\lambda & I_y I_x \\
	I_y I_x & (I_y^2+\lambda)
	\end{bmatrix}
	\cdot
	\begin{bmatrix}
	u \\
	v
	\end{bmatrix}
	=
		\begin{bmatrix}
	-I_xI_t + \lambda \bar{u}\\
	-I_yI_t + \lambda \bar{v}
		\end{bmatrix}
\end{equation}
The equation can be solved as $\boldsymbol{x} = A^{-1}b $. First to compute 
$A$'s determinant
\begin{equation}
	\frac{1}{\lambda(\lambda+I_x^2 + I_y^2)}
\end{equation}
the equation becomes:
\begin{equation}
\begin{bmatrix}
u \\
v
\end{bmatrix}
=
\frac{1}{\lambda(\lambda+I_x^2 + I_y^2)}
\begin{bmatrix}
I_y^2+\lambda & -I_y I_x \\
-I_y I_x & (I_x^2+\lambda)
\end{bmatrix}
\cdot
\begin{bmatrix}
-I_xI_t + \lambda \bar{u}\\
-I_yI_t + \lambda \bar{v}
\end{bmatrix}
\end{equation}

from this, on can easily find $u$ and $v$ as:
\begin{equation} \label{JEq}
\begin{split}
u^{k+1} = \frac{(I_{y}^2+\lambda)\bar{u}
	-I_{x}I_{y}\bar{v}
	-I_{x}I_{t}}{I_{x}^2+I_{y}^2+ \lambda}
\\
u^{k+1} = \frac{-I_{x}I_{y}\bar{u}
	+(I_{y}^2+\lambda)\bar{v}
	-I_{y}I_{t}}{I_{x}^2+I_{y}^2+ \lambda}
\end{split}
\end{equation}


\chapter{Published papers}